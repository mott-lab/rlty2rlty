**this repo is currently under construction.**
If you are interested in running this project yourself, feel free to contact me at matthew.gottsacker@ucf.edu.

# rlty2rlty

This project creates 360-degree videos that transition between two 360-degree images.
The transition optionally passes through a liminal space generated by AI.
The extended abstract describing this system will be published in the proceedings of the IEEE VR 2024 conference.
You can access a pre-print of the paper at the following link: https://sreal.ucf.edu/wp-content/uploads/2024/02/vr24d-sub1075-cam-i5.pdf.
The project demo video is available at the following link: https://www.youtube.com/watch?v=u4CyvdE3Y3g.

Examples of transitions generated with this repo that you can view on a VR headset:
- https://www.youtube.com/watch?v=cjMZkDDk20o
- https://www.youtube.com/watch?v=dRojiAOyM7k
- https://www.youtube.com/watch?v=kEyZLqNOilc
- https://www.youtube.com/watch?v=ZkDDoRO-DnE


This repo provides a browser-based interface for generating transitions between two arbitrary environments captured in 360-degree images.
It currently uses Stable Diffusion XL 1.0.
I plan to configure it to use Stable Diffusion 3 / Stable Cascade soon.


If you want to use/edit the ComfyUI workflows in your own instance of ComfyUI, the `comfyui_workflows` folder contains the workflow.json files that created the API versions called in the Python files.

## Dependencies

TODO: insert links

- ComfyUI (I use the standalone installation version)
- Python (I am using 3.11, but should work with any Python 3)
- Flask
- FFmpeg. Needs to be added to your path environment variable.

## Installation and Configuration

TODO: look into any instructions for the spatial-media-injector git submodule.

1. Clone this repo.
From the command line: `git clone https://github.com/mott-lab/rlty2rlty`
Or, download the repo as a .zip (by clicking the green button above) and extract it.

2. Install ComfyUI.

3. Configure global variables in the `app.py` Python script.
    - `COMFYUI_OUTPUT_DIR` should be set to the location that ComfyUI saves images. For me this is `ComfyUI_windows_portable/ComfyUI/output/` which I have in the same directory as the `rlty2rlty` repo. You can leave this variable unchanged if you have the same setup.s

3. Add any 360 images you want to experiment with in the `ENDENV_img/raw/` directory.
There are some example images in there already.
They should have a 2:1 aspect ratio, but they can be any resolution.
The program will automatically resize them to the proper size for processing and copy them into the `ENDENV_img/resized/` directory.

## Running

TODO: consider what should happen if user does not enter anything as a liminal prompt. LLM-generated? I guess not since that would require a GPT key... Skip liminal space?

1. Run the front-end web interface. 
Open a command line interface in the `frontend` directory and enter the command `flask run`.

2. Run the ComfyUI instance. For me, this is double-clicking the `run-nvidia-gpu.bat` program in the folder I installed it.

3. Open a web broswer and navigate to http://127.0.0.1:5000. 

4. Select the starting environment where you would like the transition video to start, and the ending environment where you would like the transition video to end up.

5. Enter a prompt for the liminal space you would like the transition video to pass through.

6. Click the `generate transition` button. 
You can observe the progress of the generation process in the command line interfaces for the rlty2rlty program and the ComfyUI program.

7. The transition video will be output in latest directory created in the `gen-dirs` directory.

## rlty2rlty Process

This section describes how the rlty2rlty transition process works.

### Image Generation Approach

The rlty2rlty system creates transition videos through multiples series of images generated based on the input 360° images of the starting environment and ending environment, as well as the user text prompt describing a liminal space.
It primarily uses the following techniques to achieve this:
- Stable Diffusion image-to-image mode
- LORA trained on 360° images
- MiDaS depth estimator
- Depth T2I-Adapter

#### Stable Diffusion img2img
The denoising strength parameters affords generating unique images based on an input image while controlling how much influence the input image has on the output.

#### LoRA

#### MiDaS Depth Estimator

#### Depth T2I-Adapter

### Transition Construction


### Limtations and Future Work

rlty2rlty is a proof-of-concept prototype designed to demonstrate using AI to generate visual transitions between two arbitrary environments without any prior knowledge of either environment.
Perhaps the most apparent limitation of this work is that it creates 360° videos, and it does not transition the meshes and textures of the 3D objects in the scene themselves.
With the rapidly developing research on neural rendering and 3D model generation, it is possible that this rlty2rlty system could be used without much alteration to generate 3D scene transitions in the near future.
In the shorter term, one solution could use a compute shader to generate approximate scene meshes based on depth images generated for each transition image, and then use a fragment shader to color the mesh based on the transition image.
I am currently working on this approach and would welcome a collaborator if you are interested in this project.
